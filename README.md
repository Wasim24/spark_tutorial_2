### spark-tutorial_2 collects small projects that I worked on using spark scala 2 Dataset/ DataFrame.  Those were first inpired by open source articles from DataBrick community, LinkedIn 'Apache Spark' User Group.  I instill my idea while rewriting. 
#### The topics include:
    1. A complete movie recommendation System using Spark-ML ALS (Alternating Least Square) alogithm 
       a) Use original MovieLens 1M Dataset (https://grouplens.org/datasets/movielens/1m/)
       b) Write python scripts to generate my PersonalRating data
       c) Parse data into Rating and Movie case classes
       d) Adopt the strategy learned from edx.org BerkeleyX course "CS120x: Distrubuted Machine Learning with Apache Spark" 
          split data into training, validation and test dataset.  Use the best model evaluated based upon validation dataset and apply it o test dataset
       e) Use ParamGridBuilder to faciliate GridSearch to find best model
       f) Augment the model with all dataset and perform transform on unrated dataset
       g) Unrated dataset was first generated by excluding movieId of PersonRating from MovieRatings then join with Movie.  
          Improve performance by using Movies dataset directly to avoid expensive join plus distinct.
       h) Avoid NaN pitfall by excluding NaN prediction data from the result from transformation.  See MovieLensALS.scala for the details of NaN pitfall.   
 
    2. Similar movie recommendation system but the source is data in MOngoDB
       a) Write simple convert.py to covert the delimiter from '::' to ',' so that I can use mongoimport to import Ratings and Movies data
       b) Use mongo-spark-connector 2.0 MongoSpark ReadConfig to load Mongo data
       c) Perform similar steps as the above ML-!M recommendation system.
       d) Mongo Spark 2 connector introduce a new bug when doing randomSplit.  The sum up of all splits does not equal to the count of whole (the toal: 1000209).  
          That distorts the results.  I opened a JIRA ticket on Mongo spark connector site to track it.

    3. Analyze Apache access log
       a) Write AccessLogParser using RegEx pattern to parse data into AccessLogRecord 
       b) Write parseDate UDF to parse access log date format to be compatible with timestamp type 
       c) Load diamond.csv using new SparkSession.read.csv with correct options 
       d) Join access log data with diamond.csv

    4. SparkSessionZipsExample to be familiar with Spark 2 Dataset/ DataFrame operation.
